---
title: "Data Manipulation"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_float: true
---

This module of the short course will focus on data import and data manipulation -- as well as the use of pipes to chain sequences of operations. Throughout, we'll emphasize the use of tidy dataframes for working with data.

```{r setup, echo = TRUE, show = "hide", message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.width = 8, 
  fig.height = 5.5,
  out.width = "90%"
)

library(tidyverse)
library(viridis)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  tibble.print_min = 3
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Slides

<script async class="speakerdeck-embed" data-id="6b337f2e2052466090e4668cd02d671f" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
<div style="margin-bottom:5px"> <strong> <a href="https://speakerdeck.com/jeffgoldsmith/aphrea-data-manipulation" title="APHREA: Data Manipulation" target="_blank">APHREA: Data Manipulation</a> </strong> from <strong><a href="https://speakerdeck.com/jeffgoldsmith" target="_blank">Jeff Goldsmith</a></strong>. </div>

Slides can be downloaded [here](./slides/ShortCourse_Data_Manipulation.pdf).

## Data Import

We're going to learn how to import [this dataset](./resources/FAS_litters.csv) -- and, by extension, many other datasets! 

### Paths

As a very quick overview, there are two kinds of paths:

* Absolute: a file or folder's "full address" on your computer
* Relative: directions to a file or folder from your current working directory

Absolute paths are often conceptually easier, because you don't really have to _think_ about them -- you're just giving the complete address, starting from the root directory. These work from any current working directory on the machine. However, absolute paths make your code extremely fragile: if you move your directory, or share your project with someone else, the **absolute** path to files will change and your code won't work. 

An absolute path example is below:

```
"/Users/jeffgoldsmith/Dropbox/Work/Talks/2022/202204_APHREA/short_course/APHREA/data/FAS_litters.csv"
```

Relative paths, meanwhile, navigate from your current working directory. Relative paths are portable, in that if you move your directory or share it with someone else, the **relative** path to files in the directory will stay the same and your code will work. For both of these reasons, relative paths are preferred in almost every setting. 

(Note: if you ever use `setwd()`, you're using absolute paths and [taking big risks](https://twitter.com/hadleywickham/status/940021008764846080)...)

The code below finds my current working directory:

```{r}
getwd()
```

A relative path to the same file as the absolute path above is

```
"./data/FAS_litters.csv"
```

One note: R Markdown files treat whatever directory they're in as the starting point for relative paths. For now, putting your R Markdown file in the same directory as you .RProj file will save a lot of headaches -- the same path will work when you knit the document and when you write / edit code interactive in the console. For larger projects, you might want to have a separate directory with several R Markdown files. In that case using [`here::here`](https://github.com/jennybc/here_here) to identify and navigate from the _project_ top-level directory is great, but you need a pretty firm understanding of paths before jumping into that. 


### Importing data tables

Now that we have paths handled, we can start loading data. We're going to start with rectangular data tables (data in rows and columns, with data separated by a delimiter) saved in a plain text format. Of these, csv (comma separated value) files are most common, and others are handled in basically the same way. To import a csv, we'll use a function from `readr` (included in the tidyverse):

```{r}
litters_data = read_csv(file = "./data/FAS_litters.csv")
```

Great -- we've imported data! The first argument to `read_csv` is the path to the data, and the line above assigns the result of `read_csv` to `litters_data`. This function call also prints information about the column parsing. We'll talk more about both of these shortly.

I (almost always) use `janitor::clean_names()` to clean up variable names after importing data. Doing so will take whatever the column names are and convert them to lower snake case.

```{r}
names(litters_data)
litters_data = janitor::clean_names(litters_data)
names(litters_data)
```

### Looking at data

The first thing to do after importing the data (unless `read_csv` gives warnings) is to look at it. If there are unexpected results during data import, you'll catch a lot of them here. In addition to printing the data (which is my first step), I often use `View` / `view`:

```{r, eval = FALSE}
view(litters_data)
```

### Other data import notes

A few quick points: 

* There are a lot of options to `read_csv()`, and these can be useful in controlling your data import results. 
* For other file formats, I use the `readxl` and `haven` packages!
* You may be used to `read.csv()` instead of `read_csv()`. There are a lot of good reasons to prefer `read_csv()`, although they aren't always readily apparent.
* You will sometimes need to export data after you have imported and cleaned it. The `write_csv()` function addresses this problem.

## Data manipulation

Once you've imported data, you're going to need to do some cleaning up.

### `select`

For a given analysis, you may only need a subset of the columns in a data table; extracting only what you need can helpfully de-clutter, especially when you have large datasets. Select columns using `select`.

You can specify the columns you want to keep by naming all of them:

```{r}
select(litters_data, group, litter_number, gd0_weight, pups_born_alive)
```

You can specify the specify a range of columns to keep:

```{r}
select(litters_data, group:gd_of_birth)
```

You can also specify columns you'd like to remove:
```{r}
select(litters_data, -pups_survive)
```

You can rename variables as part of this process:

```{r}
select(litters_data, GROUP = group, LiTtEr_NuMbEr = litter_number)
```

If all you want to do is rename something, you can use `rename` instead of `select`. This will rename the variables you care about, and keep everything else:

```{r}
rename(litters_data, GROUP = group, LiTtEr_NuMbEr = litter_number)
```

There are some handy helper functions for `select`; read about all of them using `?select_helpers`. I use `starts_with()`, `ends_with()`, and `contains()` often, especially when there variables are named with suffixes or other standard patterns:

```{r}
select(litters_data, starts_with("gd"))
```

I also frequently use is `everything()`, which is handy for reorganizing columns without discarding anything:

```{r}
select(litters_data, litter_number, pups_survive, everything())
```

`relocate` does a similar thing (and is sort of like `rename` in that it's handy but not critical):

```{r}
relocate(litters_data, litter_number, pups_survive)
```

In larger datasets, 

Lastly, like other functions in `dplyr`, `select` will export a dataframe even if you only select one column. Mostly this is fine, but sometimes you want the vector stored in the column. To pull a single variable, use `pull`.


### `filter`

Some data tables will include rows you don't need for your current analysis. Although you could remove specific row numbers using base R, you shouldn't -- this might break if the raw data are updated, and the thought process isn't transparent. Instead, you should filter rows based on logical expressions using the `filter` function. Like `select`, the first argument to `filter` is the dataframe you're filtering; all subsequent arguments are logical expressions.

You will often filter using comparison operators (`>`, `>=`, `<`, `<=`, `==`, and `!=`). You may also use `%in%` to detect if values appear in a set, and `is.na()` to find missing values. The results of comparisons are logical -- the statement is `TRUE` or `FALSE` depending on the values you compare -- and can be combined with other comparisons using the logical operators `&` and `|`, or negated using `!`. 

Some ways you might filter the litters data are:

* `gd_of_birth == 20`
* `pups_born_alive >= 2`
* `pups_survive != 4`
* `!(pups_survive == 4)`
* `group %in% c("Con7", "Con8")`
* `group == "Con7" & gd_of_birth == 20`

A very common filtering step requires you to omit missing observations. You *can* do this with `filter`, but I recommend using `drop_na` from the `tidyr` package:

* `drop_na(litters_data)` will remove any row with a missing value
* `drop_na(litters_data, wt_increase)` will remove rows for which `wt_increase` is missing. 

Filtering can be helpful for limiting a dataset to only those observations needed for an analysis. However, I recommend against the creation of many data subsets (e.g. one for each group). This can clutter up your workspace, and we'll see good tools for the analysis of subsets before long.


### `mutate`

Sometimes you need to select columns; sometimes you need to change them or create new ones. You can do this using `mutate`. 

The example below creates a new variable measuring the difference between `gd18_weight` and `gd0_weight` and modifies the existing `group` variable.

```{r}
mutate(litters_data,
  wt_gain = gd18_weight - gd0_weight,
  group = str_to_lower(group)
)
```

A few things in this example are worth noting:

* Your new variables can be functions of old variables
* New variables appear at the end of the dataset in the order that they are created
* You can overwrite old variables
* You can create a new variable and immediately refer to (or change) it

Creating a new variable that does exactly what you need can be a challenge; the more functions you know about, the easier this gets. 


### `arrange`

In comparison to the preceding, arranging is pretty straightforward. You can arrange the rows in your data according to the values in one or more columns:

```{r}
head(arrange(litters_data, group, pups_born_alive), 10)
```

### `%>%`

We've seen several commands you will use regularly for data manipulation and cleaning. You will rarely use them in isolation. For example, suppose you want to load the data, clean the column names, remove `pups_survive`, and create `wt_gain`. There are a couple of options for this kind of multi-step data manipulation:

* define intermediate datasets (or overwrite data at each stage)
* nest function calls

The following is an example of the first option:

```{r}
litters_data_raw = read_csv("./data/FAS_litters.csv")
litters_data_clean_names = janitor::clean_names(litters_data_raw)
litters_data_selected_cols = select(litters_data_clean_names, -pups_survive)
litters_data_with_vars = 
  mutate(
    litters_data_selected_cols, 
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group))
litters_data_with_vars_without_missing = 
  drop_na(litters_data_with_vars, wt_gain)
litters_data_with_vars_without_missing
```

Below, we try the second option:

```{r}
litters_data_clean = 
  drop_na(
    mutate(
      select(
        janitor::clean_names(
          read_csv("./data/FAS_litters.csv")
          ), 
      -pups_survive
      ),
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group)
    ),
  wt_gain
  )

litters_data_clean
```

These are both confusing and bad: the first gets confusing and clutters our workspace, and the second has to be read inside out.

Piping solves this problem. It allows you to turn the nested approach into a sequential chain by passing the result of one function call as an argument to the next function call:

```{r}
litters_data = 
  read_csv("./data/FAS_litters.csv") %>%
  janitor::clean_names() %>%
  select(-pups_survive) %>%
  mutate(
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group)) %>% 
  drop_na(wt_gain)

litters_data
```

All three approaches result in the same dataset, but the piped commands are by far the most straightforward. The easiest way to read `%>%` is "then"; the keyboard shortcuts are Ctrl + Shift + M (Windows) and Cmd + Shift + M (Mac).

The functions in `dplyr` (and much of the `tidyverse`) are designed to work smoothly with the pipe operator. By default, the pipe will take the result of one function call and use that as the first argument of the next function call; by design, functions in `dplyr` will take a tibble as an input and return a tibble as a result. As a consequence, functions in `dplyr` are easy to connect in a data cleaning chain. You can make this more explicit by using `.` as a placeholder for the result of the preceding call:

```{r}
litters_data = 
  read_csv("./data/FAS_litters.csv") %>%
  janitor::clean_names(dat = .) %>%
  select(.data = ., -pups_survive) %>%
  mutate(.data = .,
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group)) %>% 
  drop_na(data = ., wt_gain)
```

In this example, the dataset argument is called `dat` in `janitor::clean_names`, `.data` in the `dplyr` functions, and `data` in `drop_na` -- which is definitely confusing. In the majority of cases (and everywhere in the tidyverse) you'll elide the first argument and be happy with life, but there are some cases where the placeholder is necessary. For example, to regress `wt_gain` on `pups_born_alive`, you might use:

```{r}
litters_data %>%
  lm(wt_gain ~ pups_born_alive, data = .) %>%
  broom::tidy()
```

There are limitations to the pipe. You shouldn't have sequences that are too long; there isn't a great way to deal with multiple inputs and outputs; and (since it's not base R) not everyone will know what `%>%` means or does. That said, compared to days when R users only had the first two options, life is much better!



## Other materials

The content in this page draws heavily from several sources; each of the things below goes into more detail in one way or another.

* You can learn more about tibbles from [R for Data Science](http://r4ds.had.co.nz/tibbles.html) or the tidyverse [page](http://tibble.tidyverse.org/index.html)
* R for Data Science has a chapter on [data import](http://r4ds.had.co.nz/data-import.html)
* RStudio has recordings of webinars on [Getting data into R](https://www.rstudio.com/resources/webinars/getting-your-data-into-r/) and [What's new with readxl](https://www.rstudio.com/resources/webinars/whats-new-with-readxl/)

There's lots of stuff out there on how to clean your data using `dplyr`. 

* R for Data Science has a chapter on [data transformation](http://r4ds.had.co.nz/transform.html). The chapter on [pipes](http://r4ds.had.co.nz/pipes.html) may also be useful at this point.
* R Programming for Research also discusses [data cleaning](https://geanders.github.io/RProgrammingForResearch/entering-and-cleaning-data-1.html#data-cleaning).
* Even R Programming for Data Science, which tends towards base R, has a chapter on [`dplyr`](https://bookdown.org/rdpeng/rprogdatascience/managing-data-frames-with-the-dplyr-package.html).

